from __future__ import with_statement
from alembic import context
from sqlalchemy import engine_from_config, pool
from logging.config import fileConfig
from sqlalchemy_utils import ArrowType

# Import external tables from database
import sys
import os
basedir = os.path.dirname(os.path.realpath(__file__))
sys.path.append(basedir + '/../src/packORM')
from ar_tables import Base as ar_base
from sr_tables import Base as sr_base

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = [ar_base.metadata, sr_base.metadata]

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline():
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url, target_metadata=target_metadata, literal_binds=True, compare_type=True)

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online():
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix='sqlalchemy.',
        poolclass=pool.NullPool)

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()


# An example of how to BULK DATA INTO NEW TABLE
"""

client_credential = sa.sql.table('client_credential',
    sa.Column('client_id', sa.Integer, nullable=False),
    sa.Column('is_active', sa.Boolean, nullable=False, default=True),
    sa.Column('key', sa.String(22), nullable=False, default=True),
    sa.Column('secret', sa.String(22), nullable=False, default=True),
    sa.Column('created_at', sa.DateTime, nullable=False, default=sa.func.now()),
    sa.Column('updated_at', sa.DateTime, nullable=False, default=sa.func.now()),
)
conn = op.get_bind()
res = conn.execute("select secret, key, id from client")
results = res.fetchall()
clients = [{'secret': r[0], 'key': r[1], 'is_active':True, 'client_id': r[2], 'created_at': datetime.datetime.now(), 'updated_at': datetime.datetime.now()} for r in results]
op.bulk_insert(client_credential, clients)
op.drop_column(u'client', u'secret')
op.drop_column(u'client', u'key')

"""

# An example of how create autogenerated secuences

"""

--> 1.-Create the needed secuence
op.execute(CreateSequence(Sequence('source_evidence_id_seq', schema='city4age_ar')))
--> 2.-Create the table with the secuence
op.create_table('source_evidence',
sa.Column('id', sa.Integer(), server_default=sa.text(u"nextval('city4age_ar.source_evidence_id_seq')"), nullable=False, primary_key=True),

@@@@ whatever

"""